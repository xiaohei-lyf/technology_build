log 文件存放数据那么如何与index文件配合找到数据。
生产者如何在index维护
消费者如何在index里面维护。
当生产一条数据的流程是怎样 api
消费一条数据的流程是怎样的·

Note：SeqNumber怎么生成的？
什么时候出现丢失数据？ack设置过低
什么时候出现数据重复？当部分isr同步完成，但是leader返回ack之前挂掉

消费一致性问题？
（消费者消费到比较靠前位置但是，此时leader挂了，follower挂了，
数据靠前的位置已经被消费，偏移量出错，消息重复消费）
	出现了：hw 以及leo（log end offset）
	每个消费者最多消费到所有leader+follower 的 最低位置 即hw 位置
	
什么是消费的轮询策略？
什么是消费的range/按主题划分？
消费者组如何划分？ eg： 消费者 A 消费 topic1 topic2 ；消费者B消费了topic2 那么分组只需要 AB 分一组，关联topicA  B再分一组关联topicB。某个分区的



前置：因为topic 是做了分区的所以一个每个topic 会放在所有的服务器列表。
		消费者启动区zk注册自己地址以及关注的topic
		kafka集群启动，根据消费者关注的topic和地址做划分。
		eg：如果你只有一个消费者，并且关注至少一个topic，那么自己一组因为你需要访问所有机器。
		如果你有两个以上那么每个跟别人分一组，因为如果两个放在一起一定会产生一个分区被不止一个消费者消费。
		
		所以逻辑是：根据最大的同一个主题分几组，然后每个组里面包含，对不同主题的消费者，
		
		
		当有很多台机器的时候kafka 需要对他们进行分组以便合理的去kafka服务器列表拉取数据。
		--> 
		kafka对多个消费者进行分组，确保一个消费者组里面不会出现一个消费者消费多个额分区
		

kafka事务+精准一次性
消费者端的事务要如何实现为什么不重要

kafka做不了有序，只能保证消费的分区内有序	

如何重复消费数据 配置自动提交offect false 且不要手动提交offect每次启动就会从集群这个consumer的最大offect开始消费。
且循环获取拉取数据不会产生重复（因为消费offect本地也存了一份）	

自动提交丢失数据（拉取100条后提交offect 数据没处理完比如处理到50 挂了结果就会产生数据丢失，）
如果设置得过长比如10秒 拉取200条数据2秒处理完在第800条8秒挂掉 前面 800条又会消费一遍
		
		
		
		